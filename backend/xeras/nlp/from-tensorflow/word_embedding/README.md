## Introduction

Word embedding is an important problem in NLP. 
Word-embedding models learn to convert a word into a vector so that related words are closed together 
and a linguistic logic inferring can be replaced by a vector calculation.

Fortunately, this problem was well investigated. 
For Vietnamese, please notice tokenization module before doing word embedding. 

For these reasons, we'll make this module simple. 


You can use other open sources for word embedding as well:
FastText: https://github.com/facebookresearch/fastText

Gensim: https://github.com/RaRe-Technologies/gensim
(Docs: https://radimrehurek.com/gensim/models/word2vec.html)

